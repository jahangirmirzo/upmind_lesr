
# –£—Ä–æ–∫ 24: –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤ (–æ—Å–Ω–æ–≤—ã)

# üî∏ –ß—Ç–æ —Ç–∞–∫–æ–µ pip –∏ –∑–∞—á–µ–º –æ–Ω –Ω—É–∂–µ–Ω

# pip ‚Äî —ç—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –º–µ–Ω–µ–¥–∂–µ—Ä –ø–∞–∫–µ—Ç–æ–≤ –≤ Python.
# –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫, —Ç–∞–∫–∏—Ö –∫–∞–∫ requests –∏–ª–∏ beautifulsoup4,
# –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ –≤—Ö–æ–¥—è—Ç –≤ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –±–∏–±–ª–∏–æ—Ç–µ–∫—É —è–∑—ã–∫–∞.

# –ü—Ä–∏–º–µ—Ä —É—Å—Ç–∞–Ω–æ–≤–∫–∏ —á–µ—Ä–µ–∑ —Ç–µ—Ä–º–∏–Ω–∞–ª:
# pip install requests
# pip install beautifulsoup4

# –≠—Ç–∏ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏ –Ω—É–∂–Ω—ã –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ HTML-—Å—Ç—Ä–∞–Ω–∏—Ü ‚Äî —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏ —Ä–∞–∑–±–æ—Ä–∞ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ —Å–∞–π—Ç–∞.

# üî∏ –ß—Ç–æ —Ç–∞–∫–æ–µ –ø–∞—Ä—Å–∏–Ω–≥ (–≤–µ–±-—Å–∫—Ä–∞–ø–∏–Ω–≥)

# –ü–∞—Ä—Å–∏–Ω–≥ —Å–∞–π—Ç–æ–≤ ‚Äî —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å HTML-—Å—Ç—Ä–∞–Ω–∏—Ü.
# –û–Ω –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ –ø—Ä–æ–µ–∫—Ç–∞—Ö –¥–ª—è —Å–±–æ—Ä–∞ –Ω–æ–≤–æ—Å—Ç–µ–π, —Ü–µ–Ω —Ç–æ–≤–∞—Ä–æ–≤, –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø–æ–≥–æ–¥—ã –∏ –¥—Ä—É–≥–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏.

# üî∏ –û—Å–Ω–æ–≤–Ω—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏:
# - requests ‚Äî –ø–æ–ª—É—á–∞–µ—Ç HTML-–∫–æ–¥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
# - BeautifulSoup ‚Äî —Ä–∞–∑–±–∏—Ä–∞–µ—Ç HTML –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å –Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã.

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞:
# pip install requests beautifulsoup4

# üî∏ –ë–∞–∑–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä –∫–æ–¥–∞ –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞

import requests
from bs4 import BeautifulSoup

def basic_scraper():
    url = "https://example.com"
    try:
        response = requests.get(url)
        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")
            titles = soup.find_all("h1")
            for t in titles:
                print(t.text)
        else:
            print("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å—Ç—Ä–∞–Ω–∏—Ü—ã:", response.status_code)
    except Exception as e:
        print("–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ:", e)


# üî∏ –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç–æ–¥—ã BeautifulSoup
# find()      ‚Äî –Ω–∞—Ö–æ–¥–∏—Ç –ø–µ—Ä–≤—ã–π —Ç–µ–≥ –ø–æ —É—Å–ª–æ–≤–∏—é
# find_all()  ‚Äî –Ω–∞—Ö–æ–¥–∏—Ç –≤—Å–µ —Ç–µ–≥–∏ –ø–æ —É—Å–ª–æ–≤–∏—é
# .text       ‚Äî –∏–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ —Ç–µ–≥–∞
# [attr]      ‚Äî –ø–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∞—Ç—Ä–∏–±—É—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, href, src)

# –ü—Ä–∏–º–µ—Ä—ã:
# soup.find("p", class_="info")
# soup.find("a")["href"]

# üî∏ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã

def extract_links(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        links = soup.find_all("a")
        return [link.get("href") for link in links if link.get("href")]
    except Exception as e:
        return [f"–û—à–∏–±–∫–∞: {e}"]


# üî∏ –ü–∞—Ä—Å–∏–Ω–≥ —Ç–∞–±–ª–∏—Ü—ã

def extract_table_rows(url):
    try:
        response = requests.get(url)
        soup = BeautifulSoup(response.text, "html.parser")
        rows = soup.find_all("tr")
        return [[col.text for col in row.find_all("td")] for row in rows]
    except Exception as e:
        return [f"–û—à–∏–±–∫–∞: {e}"]


# üî∏ –≠—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–∞–≤–∏–ª–∞
# - –£–≤–∞–∂–∞–π robots.txt —Å–∞–π—Ç–∞ (–æ–Ω –º–æ–∂–µ—Ç –∑–∞–ø—Ä–µ—â–∞—Ç—å –ø–∞—Ä—Å–∏–Ω–≥)
# - –ò—Å–ø–æ–ª—å–∑—É–π –ø–∞—É–∑—ã –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ (time.sleep)
# - –ù–µ —Å–æ–∑–¥–∞–≤–∞–π –Ω–∞–≥—Ä—É–∑–∫—É –Ω–∞ —Å–µ—Ä–≤–µ—Ä
# - –í—Å–µ–≥–¥–∞ –ø—Ä–æ–≤–µ—Ä—è–π, –º–æ–∂–Ω–æ –ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–æ–±—Ä–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ

# –î–ª—è –∑–∞–ø—É—Å–∫–∞:
# basic_scraper()
# print(extract_links("https://example.com"))
